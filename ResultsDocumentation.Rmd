---
title: "Replication & Extension of Corey & Burkey"
author: "Jan Seifert"
date: "16 02 2020"
bibliography: ./pub/AveragingCorrelations.bib
csl: ./pub/apa-6th-edition.csl
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

__It is not possible to average correlation coefficients__. In my studies in the social sciences I found that in every textbook. Do not average correlation coefficients! I needed that for a project and I remembered the Fisher Z transformation [@Fisher1921] that is supposed to alleviate the bias.

First of all, mathematically it is true: the mere average of several correlation coefficients makes no sense. Correlations are not additive is the "simple" way to say it. That means something like this.

$$ {1 \over n} \cdot (r_{x_1, y_1} + r_{x_2, y_2} + ... + r_{x_n, y_n})   \ne  Correlation(x_1 ... x_n, y_1 ... y_n) $$


After some digging I found that the problem is negligible in many cases. If the sample sizes N are large enough then the bias shows only on the third digit. Even without Fisher Z "the absolute bias becomes negligible (less than .01) for a sample size greater than 20" [@Bishara2015].

Digging a little deeper I found that you can sure get a decent estimate of the mean. However, the standard deviations and, thus, the standard error may still suffer. **All kinds of significance testing may show biased results.[TODO: verify]**

Finally, I learned about other correction methods. There is one by @Hotelling1953. Another had beed suggested by @Olkin1958. @Alexander1990 summarises that the Hotelling correction is not superior to Fisher Z. The correction method by @Olkin1958 was more interesting. Not only was it more precise. I found out that @Olkin1958 did not only give us an approximate correction formula. They provided us with an [exact ... REALLY? TODO] estimate. Interestingly, nobody seemed to notice that. I did not find any paper investigating the [EXACT] approach. They always refer to the approximate one.

All these correction methods are based on three assumptions, which are 1) bivariate normality, 2) large sample sizes, and 3) independence of observations. This first story about the issue will not extend insight beyond these assumptions.

[QUESTIONS]


## About Corey, Dunlap & Burke

@Corey1998 investigated the nature of the bias when averaging correlation coefficients. In a Monte Carlo approach they investigated two different situations. They compared plain averaging and the corrected approach proposed by @Fisher1921.

1. Experiment 1: Averaging correlation coefficients from a matrix
2. Experiment 2: Averaging independent correlation coefficients

They limited their approach to correlations between normally distributed data sets.

The main questions of their study were:
* How does the bias change with the correlation in the population $\rho$?
* Does the bias change depending on **sample size N**? 
* Does it change depending on the **number of samples D** (with D for **d**ata set)? 

What measures and visualisations will I use to answer these questions when comparing various methods?

* Charts Rho $\times$ $(\rho - \bar r)$ for visual inspection.
* The ["mean absolute deviation"](https://en.wikipedia.org/w/index.php?title=Average_absolute_deviation&oldid=939296257#Mean_absolute_deviation_around_a_central_point) from zero. Here abbreviated as *zad*.
* Count where the predicted $\bar r$ is closer in one method compared to the other incl. the size and distribution of those differences
* Where appropriate I show heat maps that indicate which method is closer to $\rho$ for each combination of sample size and number of samples.



## Averaging correlation coefficients from a matrix

This paper will simulate averaged correlations with the following variations according to @Corey1998:

* A range of correlations from 0.00 to 0.90
* Averaging 3 to 45 correlations based on 3 to 10 intercorrelated data sets.
* The score pairs (N) making up a single correlation varied between 10 to 50 in steps of 10

Using these variations data sets were created, the correlation computed, and averaged afterwards. While they only used an uncorrected and a Fisher-z corrected average, this paper will add the Hotelling transformed correlations [@Hotelling1953]. Each transformed z was back-transformed to r afterwards. This process was repeated 10.000 times for each condition. The average correlations were summed up (seperately for each transformation, of course) and divded by the number of iterations.

The bias is computed as the difference between the true Rho and the averaged correlation. ... TODO

```{r Definitions, include=FALSE}
library(RColorBrewer)
library(psych)
library(gplots, quietly	= TRUE, warn.conflicts = FALSE)
source("CorrAggBias.R")

MakePlot <- function( n, m, Height = 0.05 ) {
  # N <- seq(10, 50, 10) # sample size
  D <- 3:10            # Data sets to correlate
  N <- seq(10, 50, 10)   # sample size
  R <- seq(0.00, 0.95, 0.05)  # Rho
  Palette <- brewer.pal(n = length(D)+1, name = "PuRd")[(length(D)+1):2]
  RangeY <- c(-Height, Height)
  for(d in D) {
    nChr <- as.character(n)
    dChr <- as.character(d)
    Color <- Palette[which(d==D)]
    Means <- subset(Descr, group2==nChr & group3==dChr & group4==m, select = mean)
    ScaleY <- max(abs(Means))
    if(d == D[1]) {
      plot(R, unlist(Means), ylim = RangeY, col = Color, type="l", lty=1,
           sub = n, xlab = "r", ylab = "rho - r", oma = c(1, 2, 2, 2))
      grid(nx = NA, ny = NULL, col = "lightgray")      
    }
    else
      lines(R, unlist(Means), col = Color, type="l", lty=1)
  }
  abline(h = 0, col = "gray60")
  text(0, y = min(RangeY), adj = 0, m)
}


CompareMethodsTable <- function(d1, d2) {
  Select <- c("mean", "sd", "median", "mad", "min", "max", "range")
  Description <- rbind( describe(d1$mean), describe(d2$mean) )
  Description <- Description[,Select] # drop unwanted stats
  # replace "median absolute deviation" with "zero absolute deviation
  Description[1, "mad"] <- mean(abs(d1$mean))
  Description[2, "mad"] <- mean(abs(d2$mean))
  colnames(Description)[colnames(Description) == "mad"] <- "zad"
  # determine number of values closer to zero
  ctz1 <- sum( abs(d1$mean) < abs(d2$mean) )
  ctz2 <- sum( abs(d1$mean) > abs(d2$mean) )
  Description <- cbind(Description, CtZ = c(ctz1, ctz2))
  # print
  rownames(Description) <- c(levels(d1$group4), levels(d2$group4))
  
  knitr::kable(Description,
             caption = paste(rownames(Description)),
             digits = 3)
  #print(Description, digits = 3)
}


CompareByHeat <- function(d1, d2) {
  DataLabels <- c(levels(d1$group4), levels(d2$group4))
  # compute average deviation for each Rho
  M1 <- aggregate(d1[, 8], list(Descr$group2, Descr$group3), median)
  MH <- aggregate(d2[, 8], list(Descr$group2, Descr$group3), median)
  
  MAll <- merge(M1, MH, by = c(1, 2), suffixes = paste0(".", DataLabels))
  Label1 <- paste0("x.", DataLabels[1])
  Label2 <- paste0("x.", DataLabels[2])
  data   <- matrix(unlist(abs(MAll[Label1]) - abs(MAll[Label2])), nrow=8)
  
  #Colors <- c("#8c510a", "#d8b365", "#f6e8c3", "#f5f5f5", "#c7eae5", "#5ab4ac", "#01665e")
  Colors <- c('#8c510a','#bf812d','#dfc27d','#f6e8c3','#c7eae5','#80cdc1','#35978f','#01665e')
  
  # Default Heatmap
  heatmap.2(data, scale="column", Colv = NA, Rowv = NA, dendrogram = "none",
          xlab = "Sample size", labRow = 3:10,
          ylab = "Data sets", labCol = seq(10, 50, 10),
          main = paste("Comparison", DataLabels[1], "-", DataLabels[2]),
          col = Colors, tracecol = "black",
          key.title = "Colors & Histogram", 
          key.xlab = paste0("|", DataLabels[1], "|-|", DataLabels[2], "|"))
}
```


## Replication

This study replicates @Corey1998, first. We will investigate the averaged correlations $\bar r$ and the corrected correlations using Fisher z: $\bar r_z$. It enhances the precision a little. Here we computed 50.000 calculations instead of 10.000. We also provide not only the values 0.1,0.2, etc. but add the steps in between, too.

The first plot shows the observed bias for uncorrected correlations.

```{r Replication Plot 1, echo=FALSE}
# Show replication plots
old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1)
load("./data/CoreyDunlapBurke_None_Avg.Rda")
MakePlot(50, "None", Height = 0.0045)
load("./data/CoreyDunlapBurke_Fisher_Avg.Rda")
MakePlot(50, "Fisher", Height = 0.0045)
par(old.par)
rm(old.par)
```

The uncorrected bias is largest for intermediate values of $\rho$. After correcting them the maximum bias moves down to $\rho = 0.3$. 

As expected, Fisher performs a lot better in general. The total range of the bias is only slightly smaller but the values are distributed more symmetrically around zero. Fishers average deviation from zero is a third of the uncorrected ones. It is not surprising, that Fisher returns a result that is closer to zero in 753 of 800 values. Clearly, Fisher z provides better results than the uncorrected averages.


```{r Replication Stats, echo=FALSE}
load("./data/CoreyDunlapBurke_None_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_Fisher_Avg.Rda")
d2 <- Descr
CompareMethodsTable(d1, d2)
```

Unlike the corrected correlations, the uncorrected ones do not benefit much when more samples are being used. 




## Hotelling

Hotellings correction does not get much attention in the literature. @Alexander1990 [citing @Alexander1985] claimed that Hotelling is not superior to Fisher's z. Let us compare Hotelling and Fisher z. At a first glance, it looks like Hotelling only changes the bias without doing much to alleviate it.It is extremely difficult to come to a conclusion visually.

```{r Hotelling, echo=FALSE}
old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1)
load("./data/CoreyDunlapBurke_Fisher_Avg.Rda")
MakePlot(50, "Fisher", Height = 0.0022)
load("./data/CoreyDunlapBurke_Hotelling_Avg.Rda")
MakePlot(50, "Hotelling", Height = 0.0022)
par(old.par)
rm(old.par)
```

The values in the table show that it is a close race but Hotelling is finally a nose behind. The range of values is the same and given the minimum and maximum one might hope that Hotelling offers a slight advantage because the distribution around zero might be more favourable. But in the end 8% percent of the simulated Fisher zs are closer to zero than Hotelling. The deviation from zero (zad) confirms that impression.

```{r Hotelling Stats, echo=FALSE}
load("./data/CoreyDunlapBurke_Fisher_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_Hotelling_Avg.Rda")
d2 <- Descr
CompareMethodsTable(d1, d2)
```

Because these two solutions are so close together I wanted to investigate this a little more. Is there a pattern that might help us choose one correction under specific circumstance? What we see in the heat map are the differences between Fisher and Hotelling z scores. The z scores have been aggregated with the median function giving us a combined devation for all values of $\rho$ for each combination of sample size and number of data sets. Brownish colours indicate that Fisher is closer to $\rho$ and greenish colours suggest the opposite. It is hard to say that there'd be a clear pattern. Hotelling outperforms Fisher in a few areas, especially when 4 or 5 data sets are being used. The histogram implies that advantages on the Fisher side are smaller but more widespread while Hotelling is commonly at a disadvantage but in those instances it outperforms Fisher, it does it by a lot. But is there a pattern that we can use to choose a better correction method based on the situation? The data here does not allow that conclusion.

```{r Hotelling Heatmap, echo=FALSE}
load("./data/CoreyDunlapBurke_Fisher_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_Hotelling_Avg.Rda")
d2 <- Descr
CompareByHeat(d1, d2)
```



<!-- What is mostly neglected is the fact that Hotelling gave us two equations. A simpler one $z^*$ that he suggested is appropriate for larger samples and $z^{**}$ that should be used for small ones. -->

<!-- ```{r Hotelling2, echo=FALSE} -->
<!-- old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1) -->
<!-- load("./data/CoreyDunlapBurke_Fisher_Avg.Rda") -->
<!-- MakePlot(50, "Fisher", Height = 0.01) -->
<!-- load("./data/CoreyDunlapBurke_Hotelling2_Avg.Rda") -->
<!-- MakePlot(50, "Hotelling2", Height = 0.01) -->
<!-- par(old.par) -->
<!-- rm(old.par) -->
<!-- ``` -->





## Variations of Olkin & Pratt

For one thing, authors use this correction by @Olkin1958:

$$G(r) = r \cdot \left( 1 + \frac{1-r^2}{2 \cdot (n-k)} \right)$$
with n being the degrees of freedom and $k = 3$, which @Olkin1958 used as approximation. 


### k = 3

Let us compare the $G(r)$ to the correction of Fisher. A visual inspection leaves no doubt that $G(r)$ is superior to Fishers z. The total range of values is smaller. But not only that. While other corrections show a bias that is linked to $\rho$, the $G(r)$ jumps up and down around the mean.

```{r G(k), echo=FALSE}
old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1)
load("./data/CoreyDunlapBurke_Fisher_Avg.Rda")
MakePlot(50, "Fisher", Height = 0.0022)
load("./data/CoreyDunlapBurke_MinVar_Avg.Rda")
MakePlot(50, "MinVar", Height = 0.0022)
par(old.par)
rm(old.par)

```

The numbers support that impression. The deviation around zero (zad) is less than a third. Accordingly, the less than 20% of the Fisher z values are closer to the zero than the $G(r)$ is.

```{r MinVar Stats, echo=FALSE}
load("./data/CoreyDunlapBurke_Fisher_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_MinVar_Avg.Rda")
d2 <- Descr
CompareMethodsTable(d1, d2)
```


### The actual k

In all publications (known to me) authors set k = 3. Interestingly, the exact value they provide for k is actually $k = (9 \sqrt{2} - 7) / 2 \approx 2.863961$. In 1958 and the need for pen and paper to do maths, this simplification was an improvement. With today's computing powers there is no reason not to use a more precise value. But how much more precision will we get?

```{r das wahre k}
# The actual k
(-7 + 9 * sqrt(2))/2
```

Visually it look promising. I chose a sample size of n = 10 this time. For n = 50 it was hard to detect any differences. The 'true k' bias seemed to be in a smaller range but that was all there is to see.  The range of the 'true' k bias is certainly smaller. But it is difficult to say much more. To be honest, n = 10 does not allow to dinstinguish them either. It shows an interesting property of the Olkin & Pratt correction that we have not seen before. At the smallest sample sizes they drift twoards a larger bias when $\$rho$ increases. It seems that that drift of the 'true k' develops slower leading to better results.

```{r G_Vergleich, echo=FALSE}
old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1)
load("./data/CoreyDunlapBurke_MinVar_Avg.Rda")
MakePlot(10, "MinVar", Height = 0.004)
load("./data/CoreyDunlapBurke_TrueK_Avg.Rda")
MakePlot(10, "TrueK", Height = 0.004)
par(old.par)
rm(old.par)
```


The numbers show a slight advantage of k=2.86... over k=3. The total range, the deviation from zero, the standard deviation are all smaller. All in all, 7.5% more values are closer to zero. Based on these data my conclusion is, that we should prefer the more accurate k over k=3. The advantage may be subtle but, to a computer, it makes no difference. 

```{r True k Stats, echo=FALSE}
load("./data/CoreyDunlapBurke_MinVar_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_TrueK_Avg.Rda")
d2 <- Descr
CompareMethodsTable(d1, d2)
```


### The Precise @Olkin1958

For the visual inspection I chose the sample size 10. There was not much to see for n = 50, meaning that it was not possible to see an advantage of either of the methods. Most authors agree that the averaging bias is stronger for smaller sample size and that is the reason why we see n = 10 now because that looks interesting. The approximate Olkin & Pratt estimate using the "true k" clearly reveals the drift to a larger bias when $\rho$ increases. Not so the precise version. There still is a bias and sometimes it gets even as high as 0.002 but the values stay closer to zero.

```{r G Precise Vergleich, echo=FALSE}
old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1)
load("./data/CoreyDunlapBurke_TrueK_Avg.Rda")
MakePlot(10, "TrueK", Height = 0.004)

load("./data/CoreyDunlapBurke_Precise_Avg.Rda")
MakePlot(10, "Precise", Height = 0.004)
par(old.par)
rm(old.par)
```

The numbers favour the precise algorithm, too. The total range of the bias is considerably smaller (by 40%) and for over 60% of the simulated scenarios it shows better results.

```{r Precise Stats, echo=FALSE}
load("./data/CoreyDunlapBurke_TrueK_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_Precise_Avg.Rda")
d2 <- Descr
CompareMethodsTable(d1, d2)
```

The heatmap comparison did not provide more insight which is why it is left out.

```{r Precise O&P Heatmap, include=FALSE}
load("./data/CoreyDunlapBurke_TrueK_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_Precise_Avg.Rda")
d2 <- Descr
CompareByHeat(d1, d2)
```


### Summary on @Olkin1958

@Olkin1958 offer the most exact way to estimate an $\bar r$. It should be used with caution, though. If you simply want to average the correlation, it is fine to use it. But as @Alexander1990 [p. 335] concludes, "it should not be used for other purposes in validity generalization (meta-analysis) studies since its sampling distribution suffers from the same deficiencies as does r itself".

When we use the Olkin & Pratt correction, I recommend the precise version. The average deviation from zero did not even show on the third digit. It is very slow to compute. This simulation took over a day to finish in R. Of course, normally you do not want to run the correction 40 million times and, therefore, the disadvantage in computation speed is negligible. If you need speed, use at least the 'true k'.


## Averging Determination Coefficients

@Garcia2012 fights a passionate case against averaging correlations. In a way, he is totally right. Mathematically it is just wrong to average them. And all workarounds that we found, so far, have two severe drawbacks. They provide us with a mere point estimator, the mean, but they do not consider the whole distribution. That makes statistical comparisons very difficult including confidence intervals. And they suffer when assumptions are violated, most of all the assumption of the normal distribution. 

Because of that @Garcia2012 proposed the self-weighting method. In contrast to $r$, he argues, the $r^2$ is additive.

$$ \bar r^2 = { \sum s_{y_j}^2 r_j^2 \over \sum s_{y_j}^2 } $$

### Averaging r Squared

But one thing is strange about that. If we use two bi-variate normally distributed variables and scale them to a standard deviation of 1, then the equations returns us nothing more (and nothing less) than the average of the squared correlation coefficients.

$$ 
  \bar{r}^2 = { \sum{1 r_j^2} \over \sum{1} } = { \sum r_j^2 \over {k} } \\
  r = \sqrt{\bar{r}^2}
$$

I put the equation into the same simulation as before and, see! The results are not acceptable at all. What happened here?

```{r Simple Squared Line Plot, echo=FALSE}
old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1)
load("./data/CoreyDunlapBurke_None_Avg.Rda")
MakePlot(50, "None", Height = 0.1)
load("./data/CoreyDunlapBurke_SimpleSquared_Avg.Rda")
MakePlot(50, "Squared", Height = 0.1)
par(old.par)
rm(old.par)
```

The reasons are simple. There are random fluctuations in the data. Correlations sometimes come out negative even when the correlation of the population $\rho$ is positive. Because we square them before averaging the lose their sign. They increase the estimate when they should reduce it leading to an unacceptable overestimation. Squaring the correlation simply loses the sign. That is what happens in the lower range. And that is the picture for n = 50. It gets worse with smaller sample sizes.

For larger values of $\rho$ (i.e. exceeding values of approximately 0.6) the averaging of $r^2$ underestimates $\rho$. [TODO]

```{r Simple Suared Stats, echo=FALSE}
load("./data/CoreyDunlapBurke_None_Avg.Rda")
d1 <- Descr
load("./data/CoreyDunlapBurke_SimpleSquared_Avg.Rda")
d2 <- Descr
CompareMethodsTable(d1, d2)
```

### Keeping the sign

Following a gut feeling I thought: what if we square the correlations but simply keep the sign? In what way would that improve the results? But still, we cannot recommend using squared correlations at all. Using no correction at all is still better than squaring them before averaging.

```{r Sign Squared Line Plot, echo=FALSE}
old.par <- par(mfrow = c(1, 2), mar = c(4, 3, 0.5, 1)+0.1)
load("./data/CoreyDunlapBurke_None_Avg.Rda")
MakePlot(30, "None", Height = 0.05)
load("./data/CoreyDunlapBurke_Squared_Avg.Rda")
MakePlot(30, "Squared", Height = 0.05)
par(old.par)
rm(old.par)
```



## References


